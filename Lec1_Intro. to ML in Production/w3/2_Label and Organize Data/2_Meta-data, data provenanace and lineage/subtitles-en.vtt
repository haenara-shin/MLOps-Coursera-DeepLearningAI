WEBVTT

1
00:00:00.000 --> 00:00:04.890
For some applications, having
and tracking metadata,

2
00:00:04.890 --> 00:00:08.565
data provenance, and data
lineage can be a big help.

3
00:00:08.565 --> 00:00:10.605
What do these words even mean?

4
00:00:10.605 --> 00:00:12.525
Let's look at an example.

5
00:00:12.525 --> 00:00:16.590
Here's a more complex
example of a data pipeline,

6
00:00:16.590 --> 00:00:20.310
building on our previous
example of using

7
00:00:20.310 --> 00:00:22.650
user records to
predict if someone

8
00:00:22.650 --> 00:00:25.590
is looking for a job at
a given moment in time.

9
00:00:25.590 --> 00:00:28.259
Let's say you start off
with a spam dataset.

10
00:00:28.259 --> 00:00:30.270
This may include a list of

11
00:00:30.270 --> 00:00:33.735
known spam accounts
as well as features

12
00:00:33.735 --> 00:00:35.760
such as a list of

13
00:00:35.760 --> 00:00:39.900
blacklisted IP addresses that
spammers are known to use.

14
00:00:39.900 --> 00:00:43.250
You might also implement
a learning algorithm or

15
00:00:43.250 --> 00:00:44.840
a piece of machine learning code

16
00:00:44.840 --> 00:00:47.015
and train your
learning algorithm,

17
00:00:47.015 --> 00:00:51.460
understand dataset, thus
giving you an anti-spam model.

18
00:00:51.460 --> 00:00:54.240
These arrows indicate flow

19
00:00:54.240 --> 00:00:57.225
of information or
flow of computation,

20
00:00:57.225 --> 00:00:59.070
where training your ML code on

21
00:00:59.070 --> 00:01:02.200
the spam dataset gives
you your anti-spam model.

22
00:01:02.200 --> 00:01:03.950
You then take your user data

23
00:01:03.950 --> 00:01:06.110
and apply the anti-spam model

24
00:01:06.110 --> 00:01:10.645
to it to get the
disband user data.

25
00:01:10.645 --> 00:01:14.180
We're following our usual
convention that things with

26
00:01:14.180 --> 00:01:18.085
the purple rectangle around
it represent pieces of code.

27
00:01:18.085 --> 00:01:21.235
Now, taking your
de-spammed user data.

28
00:01:21.235 --> 00:01:24.575
Next, you may want to
carry out user ID merge.

29
00:01:24.575 --> 00:01:28.380
To do that, you might start
off with some ID merged data.

30
00:01:28.380 --> 00:01:30.050
This would be labeled data

31
00:01:30.050 --> 00:01:32.660
telling you some pairs of
accounts that actually

32
00:01:32.660 --> 00:01:34.660
correspond to the same person

33
00:01:34.660 --> 00:01:37.640
have a machine learning
algorithm implementation,

34
00:01:37.640 --> 00:01:39.245
train the model on that,

35
00:01:39.245 --> 00:01:40.790
and this gives you a learned

36
00:01:40.790 --> 00:01:42.770
ID merge model that tells you

37
00:01:42.770 --> 00:01:47.000
when to combine two accounts
into a single user ID.

38
00:01:47.000 --> 00:01:50.375
You take your ID merge model,

39
00:01:50.375 --> 00:01:53.270
apply it to the
de-spammed user data.

40
00:01:53.270 --> 00:01:56.335
This gives you your
cleaned up user data.

41
00:01:56.335 --> 00:01:59.210
Then finally, based on
the clean user data,

42
00:01:59.210 --> 00:02:01.190
hopefully, some of this labels

43
00:02:01.190 --> 00:02:03.245
with whether someone's
looking for a job,

44
00:02:03.245 --> 00:02:06.709
you'll then have another
machine learning model,

45
00:02:06.709 --> 00:02:09.440
train on it to give
you a model to

46
00:02:09.440 --> 00:02:12.590
predict if a given user is
looking for a job or not.

47
00:02:12.590 --> 00:02:15.545
This is then used
to make predictions

48
00:02:15.545 --> 00:02:17.330
on other users or maybe

49
00:02:17.330 --> 00:02:19.850
across your whole
database of users.

50
00:02:19.850 --> 00:02:23.660
This level of complexity
of a data pipeline is

51
00:02:23.660 --> 00:02:27.320
not typical in large
commercial systems.

52
00:02:27.320 --> 00:02:30.700
I've seen data pipelines
or data cascades that

53
00:02:30.700 --> 00:02:33.905
are even far more
complicated than this.

54
00:02:33.905 --> 00:02:35.840
One of the challenges of

55
00:02:35.840 --> 00:02:38.480
working with data
pipelines like this is,

56
00:02:38.480 --> 00:02:41.195
what if after running
this system for months,

57
00:02:41.195 --> 00:02:44.690
you discover that, oops,

58
00:02:44.690 --> 00:02:46.730
the IP address blacklists

59
00:02:46.730 --> 00:02:49.415
you're using has
some mistakes in it.

60
00:02:49.415 --> 00:02:54.050
In particular, what if you
discover that there was

61
00:02:54.050 --> 00:03:01.785
some IP addresses that were
incorrectly blacklisted.

62
00:03:01.785 --> 00:03:07.160
Maybe because the provider
from whom you had purchased

63
00:03:07.160 --> 00:03:10.820
a blacklisted IP
addresses found out that

64
00:03:10.820 --> 00:03:15.255
there were some IP addresses
that multiple users use,

65
00:03:15.255 --> 00:03:17.310
such as multiple users on

66
00:03:17.310 --> 00:03:19.715
a corporate campus or
university campus,

67
00:03:19.715 --> 00:03:22.955
sharing IP address
for security reasons.

68
00:03:22.955 --> 00:03:26.675
But the organization
creating the blacklist

69
00:03:26.675 --> 00:03:28.790
IP address thought it
was spammy because

70
00:03:28.790 --> 00:03:31.115
so many people shared
an IP address.

71
00:03:31.115 --> 00:03:32.810
This has happened before.

72
00:03:32.810 --> 00:03:38.345
The question is, having built
up this big complex system,

73
00:03:38.345 --> 00:03:41.690
if you were to update
your spam dataset,

74
00:03:41.690 --> 00:03:44.285
won't that change
your spam model,

75
00:03:44.285 --> 00:03:46.820
and therefore that,
and therefore that,

76
00:03:46.820 --> 00:03:49.465
and therefore that,
and therefore that.

77
00:03:49.465 --> 00:03:54.420
How do you go back
and fix this problem?

78
00:03:54.420 --> 00:03:56.510
Especially if each of

79
00:03:56.510 --> 00:03:59.389
these systems was developed
by different engineers,

80
00:03:59.389 --> 00:04:02.300
and you have files spread
across the laptops of

81
00:04:02.300 --> 00:04:05.755
your machine learning
engineering development team.

82
00:04:05.755 --> 00:04:09.640
To make sure your
system is maintainable,

83
00:04:09.640 --> 00:04:11.740
especially when a piece of

84
00:04:11.740 --> 00:04:15.100
data upstream ends up
needing to be changed,

85
00:04:15.100 --> 00:04:17.770
it can be very
helpful to keep track

86
00:04:17.770 --> 00:04:21.985
of data provenance
as well as lineage.

87
00:04:21.985 --> 00:04:26.775
Data provenance refers to
where the data came from.

88
00:04:26.775 --> 00:04:31.125
Who did you purchase the
spam IP address from?

89
00:04:31.125 --> 00:04:34.720
Lineage refers to the sequence of

90
00:04:34.720 --> 00:04:39.970
steps needed to get to
the end of the pipeline.

91
00:04:39.970 --> 00:04:41.635
At the very least,

92
00:04:41.635 --> 00:04:45.430
having extensive
documentation could

93
00:04:45.430 --> 00:04:48.805
help you reconstruct data
provenance and lineage,

94
00:04:48.805 --> 00:04:50.919
but to build robust,

95
00:04:50.919 --> 00:04:54.170
maintainable systems, not in
the proof of concept stage,

96
00:04:54.170 --> 00:04:55.610
but in the production stage.

97
00:04:55.610 --> 00:04:58.250
There are more
sophisticated tools to help

98
00:04:58.250 --> 00:05:00.980
you keep track of what
happens so that you

99
00:05:00.980 --> 00:05:03.110
can change part of the system and

100
00:05:03.110 --> 00:05:05.300
hopefully replicate the rest of

101
00:05:05.300 --> 00:05:07.280
the data pipeline without

102
00:05:07.280 --> 00:05:10.155
too much unnecessary complexity.

103
00:05:10.155 --> 00:05:12.140
To be honest, the tools for

104
00:05:12.140 --> 00:05:14.240
keeping track of
data provenance and

105
00:05:14.240 --> 00:05:16.100
lineage are still immature

106
00:05:16.100 --> 00:05:17.930
into this machine learning world.

107
00:05:17.930 --> 00:05:20.540
I find that extensive
documentation can

108
00:05:20.540 --> 00:05:22.850
help and some formal tools

109
00:05:22.850 --> 00:05:24.860
like TensorFlow
Transform can also

110
00:05:24.860 --> 00:05:28.435
help but solving this
type of problem is

111
00:05:28.435 --> 00:05:30.680
still not something that we are

112
00:05:30.680 --> 00:05:33.605
great at as a community yet.

113
00:05:33.605 --> 00:05:35.675
To make life easier,

114
00:05:35.675 --> 00:05:39.650
both for managing data
pipelines as well as

115
00:05:39.650 --> 00:05:41.450
for error analysis and

116
00:05:41.450 --> 00:05:43.850
driving machine
learning development,

117
00:05:43.850 --> 00:05:46.445
there's one tip I want to share,

118
00:05:46.445 --> 00:05:50.895
which is to make extensive
use of metadata.

119
00:05:50.895 --> 00:05:55.395
Metadata is data about data.

120
00:05:55.395 --> 00:05:59.359
For example, in manufacturing
visual inspection,

121
00:05:59.359 --> 00:06:04.235
the data would be the pictures
of phones and the labels

122
00:06:04.235 --> 00:06:06.800
but if you have
metadata that tells

123
00:06:06.800 --> 00:06:10.210
you what time was this
picture of a phone taken,

124
00:06:10.210 --> 00:06:12.540
what factory was
this picture from,

125
00:06:12.540 --> 00:06:14.055
what's the line number,

126
00:06:14.055 --> 00:06:16.115
what were the camera
settings such as

127
00:06:16.115 --> 00:06:19.459
camera exposure time
and camera aperture,

128
00:06:19.459 --> 00:06:22.129
what's the number of the
phone you're inspecting,

129
00:06:22.129 --> 00:06:25.360
what's the ID of the inspector
that provided this label.

130
00:06:25.360 --> 00:06:32.715
These are examples of data
about your dataset X and Y.

131
00:06:32.715 --> 00:06:36.230
This type of metadata can
turn out to be really

132
00:06:36.230 --> 00:06:39.925
useful because if you discover,

133
00:06:39.925 --> 00:06:42.115
during machine
learning development,

134
00:06:42.115 --> 00:06:44.689
that for some strange reason,

135
00:06:44.689 --> 00:06:48.560
line number 17 in factory

136
00:06:48.560 --> 00:06:52.610
2 generates images that

137
00:06:52.610 --> 00:06:54.995
produce a lot more
errors for some reason.

138
00:06:54.995 --> 00:06:57.860
Then this allows you
to go back to see what

139
00:06:57.860 --> 00:07:01.210
was funny about line
17 and factory 2.

140
00:07:01.210 --> 00:07:03.815
But if you had not stored

141
00:07:03.815 --> 00:07:07.295
the factory in line number
metadata in the first place,

142
00:07:07.295 --> 00:07:09.290
then it would have been
really difficult to

143
00:07:09.290 --> 00:07:12.295
discover this during
error analysis.

144
00:07:12.295 --> 00:07:15.530
I found many times
when I happened

145
00:07:15.530 --> 00:07:19.400
to maybe get lucky and
store the right metadata,

146
00:07:19.400 --> 00:07:23.885
only to discover a month
later that that metadata

147
00:07:23.885 --> 00:07:25.975
helped generate a key insight

148
00:07:25.975 --> 00:07:28.890
that helped the
project move forward.

149
00:07:28.890 --> 00:07:31.970
My tip is if you have
a framework or a set

150
00:07:31.970 --> 00:07:34.835
of MLOps tools for
storing metadata,

151
00:07:34.835 --> 00:07:36.425
that will definitely make life

152
00:07:36.425 --> 00:07:39.005
easier but even if you don't,

153
00:07:39.005 --> 00:07:42.050
just like you rarely regret
commenting your code,

154
00:07:42.050 --> 00:07:44.340
I think you will
really regret storing

155
00:07:44.340 --> 00:07:47.925
metadata that could then
turn out to be useful later.

156
00:07:47.925 --> 00:07:51.440
Just like if you don't comment
your code in a timely way,

157
00:07:51.440 --> 00:07:54.670
it's much harder to go
back to comment it later.

158
00:07:54.670 --> 00:07:56.000
In the same way,

159
00:07:56.000 --> 00:07:59.300
if you don't store the
metadata in a timely way,

160
00:07:59.300 --> 00:08:01.580
it can be much
harder to go back to

161
00:08:01.580 --> 00:08:04.355
recapture and organize that data.

162
00:08:04.355 --> 00:08:08.050
One more example for
speech recognition.

163
00:08:08.050 --> 00:08:10.654
If you have audio recorded

164
00:08:10.654 --> 00:08:13.970
from different brands
of smartphones,

165
00:08:13.970 --> 00:08:15.875
let's say that in advance,

166
00:08:15.875 --> 00:08:19.445
or if you have different
labelers labeling your speech,

167
00:08:19.445 --> 00:08:23.780
or if you use a voice
activity detection model,

168
00:08:23.780 --> 00:08:25.820
then just keep track of what was

169
00:08:25.820 --> 00:08:27.260
their version number of

170
00:08:27.260 --> 00:08:30.625
the voice activity detection
model that you use.

171
00:08:30.625 --> 00:08:34.970
All of these means that
in case for some reason,

172
00:08:34.970 --> 00:08:37.100
one version of the VAD,

173
00:08:37.100 --> 00:08:38.810
voice activity detection system

174
00:08:38.810 --> 00:08:40.670
results in much larger errors,

175
00:08:40.670 --> 00:08:44.030
this significantly increases
the odds of you discovering

176
00:08:44.030 --> 00:08:45.910
that and really use

177
00:08:45.910 --> 00:08:48.970
that to improve your learning
algorithm performance.

178
00:08:48.970 --> 00:08:51.890
To summarize, metadata can be

179
00:08:51.890 --> 00:08:55.220
very useful for error
analysis and spotting

180
00:08:55.220 --> 00:08:58.565
unexpected effects or tags
or categories of data

181
00:08:58.565 --> 00:09:02.300
that have some unusually
poor performance

182
00:09:02.300 --> 00:09:03.740
or something else,

183
00:09:03.740 --> 00:09:07.120
to suggest how to
improve your system.

184
00:09:07.120 --> 00:09:09.740
Of course, maybe
not surprisingly,

185
00:09:09.740 --> 00:09:13.580
this type of metadata is
also very useful for keeping

186
00:09:13.580 --> 00:09:17.440
track of where the data came
from or data provenance.

187
00:09:17.440 --> 00:09:20.150
The takeaway from this
video is that for

188
00:09:20.150 --> 00:09:22.010
large complex machine learning

189
00:09:22.010 --> 00:09:24.244
systems that you might
need to maintain,

190
00:09:24.244 --> 00:09:26.660
keeping track of
data provenance and

191
00:09:26.660 --> 00:09:29.675
lineage can make your
life much easier.

192
00:09:29.675 --> 00:09:32.750
As part of building
out the systems,

193
00:09:32.750 --> 00:09:35.690
consider keeping
track of metadata,

194
00:09:35.690 --> 00:09:38.975
which can help you with
tracking data provenance,

195
00:09:38.975 --> 00:09:41.870
but also error analysis.

196
00:09:41.870 --> 00:09:44.270
Before we wrap up this section,

197
00:09:44.270 --> 00:09:46.935
there's just one more tip
I hope to share with you,

198
00:09:46.935 --> 00:09:48.560
which is the importance of

199
00:09:48.560 --> 00:09:50.815
balanced train dev-test splits.

200
00:09:50.815 --> 00:09:53.720
Let's go on to the next video.