WEBVTT

1
00:00:00.440 --> 00:00:02.710
For many structure data problems.

2
00:00:02.710 --> 00:00:06.999
It turns out that creating brand new
training examples is difficult, but

3
00:00:06.999 --> 00:00:11.912
there's something else you could do which
is to take existing training examples and

4
00:00:11.912 --> 00:00:16.540
figure out if there are additional
useful features you can add to it.

5
00:00:16.540 --> 00:00:18.470
Let's take a look at an example.

6
00:00:18.470 --> 00:00:23.463
Let me use an example of restaurant
recommendations where if you're

7
00:00:23.463 --> 00:00:27.938
running an app that has to
recommend restaurants to users that

8
00:00:27.938 --> 00:00:32.090
may be interested in checking
out certain restaurants.

9
00:00:32.090 --> 00:00:37.964
One way to do this would be to have a set
of features for each user of each person,

10
00:00:37.964 --> 00:00:42.948
and a set of features for each
restaurant that then get fed into some

11
00:00:42.948 --> 00:00:48.021
learning algorithms, say a neural
network and then your network,

12
00:00:48.021 --> 00:00:53.361
whose job it is to predict whether or
not this is a good recommendation,

13
00:00:53.361 --> 00:00:57.463
whether to recommend this
restaurant to that person.

14
00:00:57.463 --> 00:01:01.302
In this particular example,
which is a real example,

15
00:01:01.302 --> 00:01:06.143
error analysis showed that the system
was unfortunately frequently

16
00:01:06.143 --> 00:01:11.170
recommending to vegetarians restaurants
that only had meat options.

17
00:01:11.170 --> 00:01:12.302
There were users,

18
00:01:12.302 --> 00:01:16.901
they were pretty clearly vegetarian based
on what they had ordered before and

19
00:01:16.901 --> 00:01:21.924
the system was still sending to them maybe
a hot new restaurant that they recommended

20
00:01:21.924 --> 00:01:27.120
because there's a hot new restaurant, but
it didn't have good vegetarian options.

21
00:01:27.120 --> 00:01:30.108
So this wasn't a good experience for
anyone and

22
00:01:30.108 --> 00:01:32.740
there was a strong desire to change this.

23
00:01:32.740 --> 00:01:37.425
Now, I didn't know how to
synthesize new examples of users or

24
00:01:37.425 --> 00:01:41.841
new examples of restaurants
because this application had

25
00:01:41.841 --> 00:01:46.460
a fixed poor of users and
there are only so many restaurants.

26
00:01:46.460 --> 00:01:51.699
So rather than trying to use data
augmentation to create brand new people or

27
00:01:51.699 --> 00:01:57.269
restaurants to feed to the training set,
I thought it was more fruitful to see if

28
00:01:57.269 --> 00:02:03.120
there were features to add to either the
person inputs or to the restaurant inputs.

29
00:02:03.120 --> 00:02:07.550
Specifically one feature you can
consider adding is a feature that

30
00:02:07.550 --> 00:02:11.433
indicates whether this person
appears to be vegetarian.

31
00:02:11.433 --> 00:02:15.190
And this doesn't need to be
a binary value feature 0 1.

32
00:02:15.190 --> 00:02:20.098
It could be soft features such as
a percentage of fruit order that was

33
00:02:20.098 --> 00:02:25.630
vegetarian or some other measures of
how likely they seem to be vegetarian.

34
00:02:25.630 --> 00:02:29.370
And a feature to add on
the restaurant side would be.

35
00:02:29.370 --> 00:02:32.675
Does this restaurant have
vegetarian options or

36
00:02:32.675 --> 00:02:35.665
good vegetarian options based on the menu.

37
00:02:35.665 --> 00:02:40.537
For structure data problems,
usually you have a fixed set of users or

38
00:02:40.537 --> 00:02:44.233
a fixed set of restaurants or
fixed set of products,

39
00:02:44.233 --> 00:02:48.937
making it hard to use data augmentation or
collect new data from new

40
00:02:48.937 --> 00:02:54.000
users that you don't have yet
on restaurants that may or may not exist.

41
00:02:54.000 --> 00:02:58.966
Instead, adding features,
can be a more fruitful way to

42
00:02:58.966 --> 00:03:03.931
improve the performance of
the algorithm to fix problems

43
00:03:03.931 --> 00:03:08.400
like this one,
identify through error analysis.

44
00:03:08.400 --> 00:03:13.339
Additional features like these,
can be hand coded or they could in turn be

45
00:03:13.339 --> 00:03:18.757
generated by some learning algorithm,
such as having a learning average home,

46
00:03:18.757 --> 00:03:22.979
try to read the menu and
classify meals as vegetarian or not, or

47
00:03:22.979 --> 00:03:28.660
having people called this manually could
also work depending on your application.

48
00:03:28.660 --> 00:03:33.858
Some other food delivery examples,
we found that there were some users that

49
00:03:33.858 --> 00:03:39.240
would only ever order a tea and coffee and
some users are only ever order pizza.

50
00:03:39.240 --> 00:03:44.228
So if the product team wants to
improve the experience of these users,

51
00:03:44.228 --> 00:03:49.131
a machine learning team might ask
what are the additional features we

52
00:03:49.131 --> 00:03:53.776
can add to detect who are the people
that only order tea or coffee or

53
00:03:53.776 --> 00:03:59.140
who are the people that only ever or
the pizza and enrich the user features.

54
00:03:59.140 --> 00:04:03.494
So as the hope the learning algorithm
make better recommendations for

55
00:04:03.494 --> 00:04:07.240
restaurants that these
users may be interested in.

56
00:04:07.240 --> 00:04:09.208
Over the last several years,

57
00:04:09.208 --> 00:04:13.459
there's been a trend in product
recommendations of a shift from

58
00:04:13.459 --> 00:04:19.540
collaborative filtering approaches to
what content based filtering approaches.

59
00:04:19.540 --> 00:04:24.056
Collaborative filtering approaches
is loosely an approach that looks at

60
00:04:24.056 --> 00:04:27.692
the user, tries to figure out
who is similar to that user and

61
00:04:27.692 --> 00:04:31.590
then recommends things to you
that people like you also liked.

62
00:04:31.590 --> 00:04:36.444
In contrast, a content based filtering
approach will tend to look at you

63
00:04:36.444 --> 00:04:41.379
as a person and look at the description
of the restaurant or look at the menu

64
00:04:41.379 --> 00:04:46.236
of the restaurants and look at other
information about the restaurant,

65
00:04:46.236 --> 00:04:49.930
to see if that restaurant is
a good match for you or not.

66
00:04:49.930 --> 00:04:55.762
The advantage of content based filtering
is that even if there's a new restaurant

67
00:04:55.762 --> 00:05:00.832
or a new product that hardly anyone
else has liked by actually looking at

68
00:05:00.832 --> 00:05:05.987
the description of the restaurant,
rather than just looking at who else

69
00:05:05.987 --> 00:05:11.251
like the restaurants, you can more
quickly make good recommendations.

70
00:05:11.251 --> 00:05:15.560
This is sometimes also called
the Cold Start Problem.

71
00:05:15.560 --> 00:05:20.692
How do you recommend a brand new product
that almost no one else has purchased or

72
00:05:20.692 --> 00:05:22.270
like or dislike so far?

73
00:05:22.270 --> 00:05:28.323
And one of the ways to do that is to make
sure that you capture good features for

74
00:05:28.323 --> 00:05:31.940
the things that you
might want to recommend.

75
00:05:31.940 --> 00:05:34.467
Unlike collaborative filtering, which

76
00:05:34.467 --> 00:05:39.373
requires a bunch of people to look at the
product and decide if they like it or not,

77
00:05:39.373 --> 00:05:44.240
before it can decide whether a new user
should be recommended the same product.

78
00:05:44.240 --> 00:05:49.960
So data iteration for structured
data problems may look like this.

79
00:05:49.960 --> 00:05:57.240
You start out with some model, train the
model and then carry out error analysis.

80
00:05:57.240 --> 00:06:02.113
Error analysis can be harder on
structured data problems if there is no

81
00:06:02.113 --> 00:06:06.655
good baseline such as human level
performance to compare to, and

82
00:06:06.655 --> 00:06:11.694
human level performance is hard for
structure data because it's really

83
00:06:11.694 --> 00:06:16.760
difficult for people to recommend
good restaurants even to each other.

84
00:06:16.760 --> 00:06:22.244
But I found that error analysis can
discover ideas for improvement,

85
00:06:22.244 --> 00:06:27.180
so can user feedback and so
can benchmarking to competitors.

86
00:06:27.180 --> 00:06:31.564
But through these methods,
if you can identify a academy or

87
00:06:31.564 --> 00:06:37.353
a certain type of tag associated your
data that you want to drive improvement,

88
00:06:37.353 --> 00:06:41.824
then you may be able to go back
to select some features to add,

89
00:06:41.824 --> 00:06:47.350
such as features to figure out who's
vegetarian and what restaurants have

90
00:06:47.350 --> 00:06:52.390
good vegetarian options that would
help you to improve your model.

91
00:06:52.390 --> 00:06:57.228
And because the specific application may
have only a finite list of users and

92
00:06:57.228 --> 00:07:02.218
restaurants, the users and restaurants
you have maybe all the data you have,

93
00:07:02.218 --> 00:07:05.181
which is why adding
features to the examples.

94
00:07:05.181 --> 00:07:10.029
You have maybe a more fruitful approach
compared to trying to come up with

95
00:07:10.029 --> 00:07:12.110
new users or new restaurants.

96
00:07:12.110 --> 00:07:17.039
And of course I think features
are a form of data to which

97
00:07:17.039 --> 00:07:21.970
is why this form of data
iteration where error analysis

98
00:07:21.970 --> 00:07:26.540
helps you decide how to
modify the features.

99
00:07:26.540 --> 00:07:30.206
That can be an efficient way as
well of improving your learning

100
00:07:30.206 --> 00:07:32.240
albums performance.

101
00:07:32.240 --> 00:07:36.812
I know that many years ago before the rise
of deep Learning, part of the hope for

102
00:07:36.812 --> 00:07:41.440
deep learning was that you don't have
to hand design features anymore.

103
00:07:41.440 --> 00:07:46.311
I think that has for the most part come
true for unstructured data problems.

104
00:07:46.311 --> 00:07:49.610
So I used to hand design features for
images.

105
00:07:49.610 --> 00:07:51.050
I just don't do that anymore.

106
00:07:51.050 --> 00:07:53.140
Let the learning I won't figure it out.

107
00:07:53.140 --> 00:07:56.071
But even with the rise
of modern deep learning,

108
00:07:56.071 --> 00:08:00.691
if your dataset size isn't massive,
there is still designing of features

109
00:08:00.691 --> 00:08:05.640
driven by error analysis that can be
useful for many applications today.

110
00:08:05.640 --> 00:08:09.314
The larger data set, the more
likely it is that a pure enter and

111
00:08:09.314 --> 00:08:11.540
deep learning album can work.

112
00:08:11.540 --> 00:08:16.463
But for anyone other than the largest tech
companies and sometimes even them for

113
00:08:16.463 --> 00:08:21.315
some applications, designing features,
especially for structured data,

114
00:08:21.315 --> 00:08:26.120
problems can still be a very important
driver of performance improvements.

115
00:08:26.120 --> 00:08:28.034
Maybe just don't do that for

116
00:08:28.034 --> 00:08:32.899
unstructured data in the as much because
learning items are very good and

117
00:08:32.899 --> 00:08:37.950
learning features automatically for
images, audio and for a text maybe.

118
00:08:37.950 --> 00:08:42.351
But for structure data is okay to
go in and work on the features.