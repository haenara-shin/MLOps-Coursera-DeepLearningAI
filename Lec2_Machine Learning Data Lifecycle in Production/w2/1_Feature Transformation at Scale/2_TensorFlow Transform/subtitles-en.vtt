WEBVTT

1
00:00:00.000 --> 00:00:03.570
To do pre-processing at scale,

2
00:00:03.570 --> 00:00:05.475
we need good tools.

3
00:00:05.475 --> 00:00:07.740
TensorFlow Transform is one

4
00:00:07.740 --> 00:00:10.260
of the best tools
available today.

5
00:00:10.260 --> 00:00:13.320
Let's take a look. First of all,

6
00:00:13.320 --> 00:00:15.000
here's what we're going
to be talking about.

7
00:00:15.000 --> 00:00:19.530
We are going to go a bit deeper
into how transform works,

8
00:00:19.530 --> 00:00:22.305
what it does, and why it does it.

9
00:00:22.305 --> 00:00:24.660
We are going to look at
the benefits of using

10
00:00:24.660 --> 00:00:26.730
TensorFlow Transform and how

11
00:00:26.730 --> 00:00:29.265
it applies feature
transformations,

12
00:00:29.265 --> 00:00:31.740
and we are going to look
at some of the analyzers,

13
00:00:31.740 --> 00:00:36.465
and the role they play
in doing pre-processing.

14
00:00:36.465 --> 00:00:39.475
Let's talk about transform.

15
00:00:39.475 --> 00:00:44.030
First off, it does what you
might think it would do,

16
00:00:44.030 --> 00:00:45.850
it takes Training Data,

17
00:00:45.850 --> 00:00:47.970
and it's going to process it.

18
00:00:47.970 --> 00:00:49.430
The end result is going to be

19
00:00:49.430 --> 00:00:52.400
deployed into the Serving System.

20
00:00:52.400 --> 00:00:55.385
In the middle, there's a
PIPELINE that is used,

21
00:00:55.385 --> 00:01:00.470
and there is METADATA that
forms a key role in organizing

22
00:01:00.470 --> 00:01:02.900
and managing the
artifacts that are

23
00:01:02.900 --> 00:01:06.005
produced as data is transformed.

24
00:01:06.005 --> 00:01:08.150
One of the reasons that's
important, is because,

25
00:01:08.150 --> 00:01:10.474
we want to understand the lineage

26
00:01:10.474 --> 00:01:13.115
or provenance of those artifacts,

27
00:01:13.115 --> 00:01:15.260
and be able to connect them,

28
00:01:15.260 --> 00:01:16.610
and find the chain of

29
00:01:16.610 --> 00:01:19.750
operations that
generated each of them.

30
00:01:19.750 --> 00:01:21.805
We have our Training Data,

31
00:01:21.805 --> 00:01:23.570
that is the input data into

32
00:01:23.570 --> 00:01:26.080
the system that
forms an artifact,

33
00:01:26.080 --> 00:01:31.265
we give that to Transform
and it transforms our data.

34
00:01:31.265 --> 00:01:34.415
It's going to take our
raw data and move it into

35
00:01:34.415 --> 00:01:35.810
the form that we're
actually going to

36
00:01:35.810 --> 00:01:37.775
use at our feature engineering.

37
00:01:37.775 --> 00:01:40.895
That's given to the Trainer

38
00:01:40.895 --> 00:01:42.770
which is going to do training.

39
00:01:42.770 --> 00:01:44.870
Transform and Trainer here are

40
00:01:44.870 --> 00:01:47.735
both components in a ML pipeline,

41
00:01:47.735 --> 00:01:52.280
specifically a TFX ML pipeline.

42
00:01:52.280 --> 00:01:55.725
These are the Trainer, of
course as a Trained Model,

43
00:01:55.725 --> 00:01:58.275
that is also an artifact.

44
00:01:58.275 --> 00:02:00.140
That gets delivered to

45
00:02:00.140 --> 00:02:03.215
the serving system or
whatever system we're using,

46
00:02:03.215 --> 00:02:06.050
where it's going to be
used to run inference.

47
00:02:06.050 --> 00:02:08.030
Looking at this a
little differently,

48
00:02:08.030 --> 00:02:10.160
we're starting with our
training and eval data.

49
00:02:10.160 --> 00:02:12.515
We've actually split our dataset.

50
00:02:12.515 --> 00:02:15.620
We split it with ExampleGen,

51
00:02:15.620 --> 00:02:18.305
so ExampleGen does that split.

52
00:02:18.305 --> 00:02:21.060
Those get fed to StatisticsGen.

53
00:02:21.060 --> 00:02:26.670
These are both TFX components
within a TFX pipeline,

54
00:02:26.670 --> 00:02:28.680
so ExampleGen is component,

55
00:02:28.680 --> 00:02:30.555
StatisticsGen is a component.

56
00:02:30.555 --> 00:02:34.370
StatisticsGen calculates
statistics for our data.

57
00:02:34.370 --> 00:02:36.080
For each of the features,

58
00:02:36.080 --> 00:02:37.880
if they're numeric features,

59
00:02:37.880 --> 00:02:42.065
for example, what is the
mean of that feature value?

60
00:02:42.065 --> 00:02:44.135
What is the standard deviation?

61
00:02:44.135 --> 00:02:46.720
The min, the max, so forth.

62
00:02:46.720 --> 00:02:49.680
Those statistics get fed to

63
00:02:49.680 --> 00:02:54.335
SchemaGen which infers the
types of each of our features.

64
00:02:54.335 --> 00:02:57.260
That creates a schema
that is then used by

65
00:02:57.260 --> 00:03:01.490
downstream components
including Example Validator,

66
00:03:01.490 --> 00:03:04.610
which takes those
statistics and that schema,

67
00:03:04.610 --> 00:03:07.145
and it looks for
problems in our data.

68
00:03:07.145 --> 00:03:09.110
We won't go into
great detail here

69
00:03:09.110 --> 00:03:10.940
about the things
that it looks for,

70
00:03:10.940 --> 00:03:13.550
but if we have

71
00:03:13.550 --> 00:03:16.960
examples that are the wrong
type in a particular feature,

72
00:03:16.960 --> 00:03:21.020
so maybe we have an integer
where we expected a float.

73
00:03:21.020 --> 00:03:23.210
Now, we're getting
into transform.

74
00:03:23.210 --> 00:03:26.420
Transform is going to
take the schema that was

75
00:03:26.420 --> 00:03:30.530
generated on the
original split dataset,

76
00:03:30.530 --> 00:03:33.620
and it's going to do our
feature engineering.

77
00:03:33.620 --> 00:03:36.925
Transform is where the
feature engineering happen.

78
00:03:36.925 --> 00:03:39.029
That gets given the Trainer,

79
00:03:39.029 --> 00:03:42.150
there's Evaluator that
evaluates the results,

80
00:03:42.150 --> 00:03:43.560
a set of Pusher that pushes it

81
00:03:43.560 --> 00:03:46.430
to our deployment
targets which is,

82
00:03:46.430 --> 00:03:48.350
however we're serving our models,

83
00:03:48.350 --> 00:03:51.750
so TENSORFLOW SERVING, or JS,

84
00:03:51.750 --> 00:03:56.365
or LITE, wherever it is that
we're serving our model.

85
00:03:56.365 --> 00:03:59.015
Internally, if we want to look

86
00:03:59.015 --> 00:04:01.339
at the transform component,

87
00:04:01.339 --> 00:04:03.665
it's getting inputs from,

88
00:04:03.665 --> 00:04:06.685
as we saw, ExampleGen
and SchemaGen.

89
00:04:06.685 --> 00:04:08.600
That is the data that was

90
00:04:08.600 --> 00:04:11.135
originally split by Example Gen,

91
00:04:11.135 --> 00:04:14.015
and the schema that was
generated by SchemaGen.

92
00:04:14.015 --> 00:04:15.740
That schema, by the way,

93
00:04:15.740 --> 00:04:18.860
may very well have been reviewed

94
00:04:18.860 --> 00:04:22.070
and improved by a developer

95
00:04:22.070 --> 00:04:25.340
who knew more about
what to expect

96
00:04:25.340 --> 00:04:29.120
from the data than can be
really inferred by SchemaGen.

97
00:04:29.120 --> 00:04:32.155
That's referred to as
curating the schema.

98
00:04:32.155 --> 00:04:35.100
Transform gets that and it

99
00:04:35.100 --> 00:04:37.530
also gets a lot of
user code because

100
00:04:37.530 --> 00:04:42.600
we need to express the feature
engineering we want to do.

101
00:04:42.600 --> 00:04:44.625
If we're going to
normalize a feature,

102
00:04:44.625 --> 00:04:46.320
we need to give it user code to

103
00:04:46.320 --> 00:04:48.345
tell transform to do that.

104
00:04:48.345 --> 00:04:52.200
The result is a TensorFlow graph,

105
00:04:52.200 --> 00:04:53.590
which were referred to as

106
00:04:53.590 --> 00:04:57.869
the transform graph and
the transform data itself.

107
00:04:57.869 --> 00:05:00.460
The graph expresses all of

108
00:05:00.460 --> 00:05:04.745
the transformations that
we are doing on our data,

109
00:05:04.745 --> 00:05:07.020
has a TensorFlow graph.

110
00:05:07.020 --> 00:05:09.900
The transform data is simply

111
00:05:09.900 --> 00:05:12.825
the result of doing all
those transformations.

112
00:05:12.825 --> 00:05:14.745
Those are given to trainer,

113
00:05:14.745 --> 00:05:15.995
which is going to use

114
00:05:15.995 --> 00:05:18.720
the transformed data
for training and

115
00:05:18.720 --> 00:05:19.750
it's going to include

116
00:05:19.750 --> 00:05:21.365
the transform graph and

117
00:05:21.365 --> 00:05:23.250
we'll take a look at
that in a second.

118
00:05:23.250 --> 00:05:25.700
We have a user provided

119
00:05:25.700 --> 00:05:29.505
transform component and
we have a schema for it.

120
00:05:29.505 --> 00:05:32.505
We apply our transformations

121
00:05:32.505 --> 00:05:36.000
during training and
as we'll see later,

122
00:05:36.000 --> 00:05:39.930
we also apply those
transformations at serving time.

123
00:05:39.930 --> 00:05:42.705
There's a lot of
perform optimizations

124
00:05:42.705 --> 00:05:44.170
that we apply as well,

125
00:05:44.170 --> 00:05:46.635
and we'll take a look
at that in a second.

126
00:05:46.635 --> 00:05:50.090
Continuing a little deeper here,

127
00:05:50.090 --> 00:05:55.170
we have our raw data and we
have our transform component.

128
00:05:55.170 --> 00:05:58.245
It produces a TensorFlow graph.

129
00:05:58.245 --> 00:05:59.850
The result of running

130
00:05:59.850 --> 00:06:03.375
that graph is processed
data, it's been transformed.

131
00:06:03.375 --> 00:06:06.749
That gets given in
model training,

132
00:06:06.749 --> 00:06:10.625
as processed data is given
to our trainer component,

133
00:06:10.625 --> 00:06:13.335
where we use it for
training our model.

134
00:06:13.335 --> 00:06:16.635
Training our model creates
a TensorFlow graph.

135
00:06:16.635 --> 00:06:19.260
Notice now we have
two different graphs.

136
00:06:19.260 --> 00:06:21.670
We have a graph here from

137
00:06:21.670 --> 00:06:25.185
transform and a graph
here from training.

138
00:06:25.185 --> 00:06:28.360
Using the Tf Transform API,

139
00:06:28.360 --> 00:06:31.000
we expressed the feature
engineering that we want to

140
00:06:31.000 --> 00:06:34.890
do and we give that code,

141
00:06:34.890 --> 00:06:38.520
or rather the transform
component gives that code to

142
00:06:38.520 --> 00:06:43.875
a Apache Beam distributed
processing cluster.

143
00:06:43.875 --> 00:06:45.660
That way we can do,

144
00:06:45.660 --> 00:06:47.820
and remember this is
all designed to work

145
00:06:47.820 --> 00:06:51.310
with potentially
terabytes of data.

146
00:06:51.380 --> 00:06:54.520
That could be quite a
bit of processing to

147
00:06:54.520 --> 00:06:57.420
do all of that
transformation using

148
00:06:57.420 --> 00:07:00.645
a distributed processing
cluster by using

149
00:07:00.645 --> 00:07:04.580
Apache Beam gives us the
capacity to do that.

150
00:07:04.580 --> 00:07:08.210
The result is a saved model.

151
00:07:08.210 --> 00:07:10.995
Saved model is just a
format that expresses

152
00:07:10.995 --> 00:07:14.660
a trained model as saved model.

153
00:07:14.660 --> 00:07:17.670
That gets included, we have

154
00:07:17.670 --> 00:07:22.930
both the transform graph
and the training graph.

155
00:07:22.930 --> 00:07:27.195
Those are given to our
serving infrastructure.

156
00:07:27.195 --> 00:07:29.195
Now we have both of those graphs,

157
00:07:29.195 --> 00:07:32.040
the feature engineering
that we did,

158
00:07:32.040 --> 00:07:33.570
and the model itself,

159
00:07:33.570 --> 00:07:35.460
the trained model and

160
00:07:35.460 --> 00:07:38.670
those are used when
we serve requests.

161
00:07:38.670 --> 00:07:43.670
Looking at this a little
differently, we have analyzers.

162
00:07:45.440 --> 00:07:48.365
Again, we're using TensorFlow and

163
00:07:48.365 --> 00:07:50.849
we're using the
TensorFlow Transform API.

164
00:07:50.849 --> 00:07:53.310
We're using TensorFlow Ops

165
00:07:53.310 --> 00:07:55.865
and a big part of what they do,

166
00:07:55.865 --> 00:07:57.365
what an analyzer does is it makes

167
00:07:57.365 --> 00:08:01.235
a full pass over our
dataset in order

168
00:08:01.235 --> 00:08:04.080
to collect constants that

169
00:08:04.080 --> 00:08:07.185
we're going to need when
we do feature engineering.

170
00:08:07.185 --> 00:08:10.980
For example, if we're going
to do a min-max, well,

171
00:08:10.980 --> 00:08:14.495
we need to make a full pass
through our data to know what

172
00:08:14.495 --> 00:08:16.485
the min and the max

173
00:08:16.485 --> 00:08:19.565
are for each feature that
we're using for that.

174
00:08:19.565 --> 00:08:23.520
Those are constants that
we need to express.

175
00:08:23.520 --> 00:08:26.800
We're going to use an
analyzer to make that

176
00:08:26.800 --> 00:08:30.285
pass over the data and
collect those constants.

177
00:08:30.285 --> 00:08:32.190
It's also going to express

178
00:08:32.190 --> 00:08:35.045
the operations that
we're going to do.

179
00:08:35.045 --> 00:08:37.950
They behave like TensorFlow Ops,

180
00:08:37.950 --> 00:08:40.380
but they only were on once during

181
00:08:40.380 --> 00:08:45.150
training and then they're
saved off as a graph.

182
00:08:45.150 --> 00:08:50.130
For example, if we're
using the tft.min,

183
00:08:50.130 --> 00:08:53.974
which is one of the methods
in the transform STK,

184
00:08:53.974 --> 00:08:56.105
it'll compute the minimum of

185
00:08:56.105 --> 00:08:59.140
a tensor over the
training dataset.

186
00:09:00.140 --> 00:09:03.225
Looking at how that gets applied,

187
00:09:03.225 --> 00:09:07.790
we have the graph that is
our feature engineering.

188
00:09:07.790 --> 00:09:11.160
We run an analysis
across our dataset to

189
00:09:11.160 --> 00:09:14.885
collect the constants
that we need.

190
00:09:14.885 --> 00:09:18.380
Really what we're doing
is we're enabling us

191
00:09:18.380 --> 00:09:22.745
later to apply these into
our Transform graph,

192
00:09:22.745 --> 00:09:26.455
so that we can transform
individual examples

193
00:09:26.455 --> 00:09:30.950
without making a pass
over our entire dataset.

194
00:09:30.950 --> 00:09:33.735
That gets applied
during training,

195
00:09:33.735 --> 00:09:38.615
and the same exact graph
gets applied during serving.

196
00:09:38.615 --> 00:09:41.410
There is no potential
here for training

197
00:09:41.410 --> 00:09:44.045
and serving skew or having

198
00:09:44.045 --> 00:09:46.150
different code paths when we

199
00:09:46.150 --> 00:09:48.800
train our model
versus when we serve

200
00:09:48.800 --> 00:09:51.350
our model that cause problems

201
00:09:51.350 --> 00:09:55.225
and those not be equivalent.

202
00:09:55.225 --> 00:09:58.375
We're using exactly the
same graph in both places,

203
00:09:58.375 --> 00:10:01.380
so there is no potential
for doing that.

204
00:10:01.900 --> 00:10:03.990
What is the benefits of that?

205
00:10:03.990 --> 00:10:08.485
Well, the emitted graph
that Transform emits

206
00:10:08.485 --> 00:10:11.455
is it has all necessary constants

207
00:10:11.455 --> 00:10:14.320
and transformations that
we're going to need.

208
00:10:14.320 --> 00:10:16.020
The focus is on data

209
00:10:16.020 --> 00:10:19.205
pre-processing only
at training time.

210
00:10:19.205 --> 00:10:21.190
It's doing that processing

211
00:10:21.190 --> 00:10:23.590
as well as generating that graph.

212
00:10:23.590 --> 00:10:27.360
Have works in-line during
both training and serving,

213
00:10:27.360 --> 00:10:29.980
we prepend it to
our trained model.

214
00:10:29.980 --> 00:10:32.310
There's really no need

215
00:10:32.310 --> 00:10:35.230
for pre-processing
code at serving time.

216
00:10:35.230 --> 00:10:38.365
It consistently applies
those transformations

217
00:10:38.365 --> 00:10:41.384
irrespective of the
deployment platform.

218
00:10:41.384 --> 00:10:43.520
Remember, we could be
deploying our model

219
00:10:43.520 --> 00:10:46.120
to a TensorFlow server

220
00:10:46.120 --> 00:10:49.190
or a mobile device using

221
00:10:49.190 --> 00:10:53.580
TensorFlow Lite or a web
browser using TensorFlow Jazz.

222
00:10:53.580 --> 00:10:57.555
Using the same TensorFlow
graph in all places,

223
00:10:57.555 --> 00:11:01.200
we're going to get exactly
the same transformations.

224
00:11:01.200 --> 00:11:04.375
The Analyzers framework itself

225
00:11:04.375 --> 00:11:06.235
has some different aspects.

226
00:11:06.235 --> 00:11:08.560
First of all, you can do scaling.

227
00:11:08.560 --> 00:11:11.390
Things like scaling
with a z-score or just

228
00:11:11.390 --> 00:11:14.325
scaling between zero
and one, bucketizing.

229
00:11:14.325 --> 00:11:17.410
We're going to bucketize
the quantiles, for example.

230
00:11:17.410 --> 00:11:19.555
We're going to set up a
set of buckets and then

231
00:11:19.555 --> 00:11:21.530
we're going to take
those values that

232
00:11:21.530 --> 00:11:23.905
we have and assign them to
the bucket so that we have

233
00:11:23.905 --> 00:11:26.760
categorical values
for numerical ranges

234
00:11:26.760 --> 00:11:29.415
of value or vocabularies.

235
00:11:29.415 --> 00:11:31.770
Things like running
[inaudible] for example,

236
00:11:31.770 --> 00:11:34.105
bag of words or n-grams.

237
00:11:34.105 --> 00:11:37.455
Intact processing need
to work with vocabulary.

238
00:11:37.455 --> 00:11:39.780
You could do dimensionality
reduction too.

239
00:11:39.780 --> 00:11:41.904
You can do a PCA Transform

240
00:11:41.904 --> 00:11:45.775
to reduce the dimensionality
of your data.

241
00:11:45.775 --> 00:11:48.220
Now let's look at some code.

242
00:11:48.220 --> 00:11:51.410
We're going to create a
pre-processing function.

243
00:11:51.410 --> 00:11:54.840
This is the entry point
that we're going to use to

244
00:11:54.840 --> 00:11:56.965
define the user code that

245
00:11:56.965 --> 00:12:00.255
expresses the feature
engineering they're going to do.

246
00:12:00.255 --> 00:12:03.100
For example, we may make

247
00:12:03.100 --> 00:12:06.990
a pass over our data
to look for floats.

248
00:12:06.990 --> 00:12:09.020
For our floats, we're going to

249
00:12:09.020 --> 00:12:10.945
scale those using a z-score.

250
00:12:10.945 --> 00:12:12.809
This is just an example.

251
00:12:12.809 --> 00:12:14.010
You're going to do whatever

252
00:12:14.010 --> 00:12:15.450
feature engineering
you have to do,

253
00:12:15.450 --> 00:12:20.160
but it's this style of code
that you're working with,

254
00:12:20.160 --> 00:12:21.410
and this is of course, Python.

255
00:12:21.410 --> 00:12:24.875
A vocabulary very similar.

256
00:12:24.875 --> 00:12:29.355
If we have vocabulary features

257
00:12:29.355 --> 00:12:31.990
or that we want to
use with vocabulary,

258
00:12:31.990 --> 00:12:33.765
we would do this thing.

259
00:12:33.765 --> 00:12:36.690
These constants here are lists of

260
00:12:36.690 --> 00:12:39.355
the keys for each of

261
00:12:39.355 --> 00:12:40.825
the features that we want a grip

262
00:12:40.825 --> 00:12:43.830
into that particular
transformation.

263
00:12:43.830 --> 00:12:47.355
Bucket features,
exactly the same thing.

264
00:12:47.355 --> 00:12:50.040
That bucket feature
key is constant,

265
00:12:50.040 --> 00:12:51.890
is just a list of

266
00:12:51.890 --> 00:12:55.285
the keys for the features
that we want to bucketize.

267
00:12:55.285 --> 00:12:57.240
Imports, well, you're going

268
00:12:57.240 --> 00:12:59.485
to import TensorFlow, obviously.

269
00:12:59.485 --> 00:13:02.130
Apache Beam, you're going to
need that because remember,

270
00:13:02.130 --> 00:13:03.360
Transform is going to use

271
00:13:03.360 --> 00:13:07.125
Apache Beam to distribute
processing across a cluster.

272
00:13:07.125 --> 00:13:08.970
Now you can also just use Beam on

273
00:13:08.970 --> 00:13:12.420
a single system or you can
just run it on your laptop.

274
00:13:12.420 --> 00:13:15.410
It has something called
the DirectRunner.

275
00:13:15.410 --> 00:13:18.060
You don't need to actually
set up a cluster.

276
00:13:18.060 --> 00:13:20.280
You can just run it,
and there's nothing

277
00:13:20.280 --> 00:13:23.000
to move infrastructure to setup.

278
00:13:23.000 --> 00:13:25.835
In development that's
pretty useful.

279
00:13:25.835 --> 00:13:27.360
In a deployment, unless you're

280
00:13:27.360 --> 00:13:29.145
working with a small
amount of data,

281
00:13:29.145 --> 00:13:32.770
you probably want more
compute horsepower than that,

282
00:13:32.770 --> 00:13:36.715
apache_beam.io.iobase
is part of that.

283
00:13:36.715 --> 00:13:39.030
It helps us with IO in

284
00:13:39.030 --> 00:13:42.855
different formats and different
ways reading and writing.

285
00:13:42.855 --> 00:13:45.900
TensorFlow Transform
itself, obviously.

286
00:13:45.900 --> 00:13:50.085
Often we use the abbreviation
of TFT for that.

287
00:13:50.085 --> 00:13:55.840
Then Transform itself has a
Beam part of its modules.

288
00:13:55.840 --> 00:13:59.220
It's often nice to pull out
separately and work with.

289
00:13:59.220 --> 00:14:02.280
Those are usually
the basic imports

290
00:14:02.280 --> 00:14:05.410
that we need to work
with Transform.