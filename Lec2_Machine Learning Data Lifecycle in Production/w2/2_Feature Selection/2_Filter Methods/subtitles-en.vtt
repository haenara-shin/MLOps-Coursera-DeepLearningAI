WEBVTT

1
00:00:00.000 --> 00:00:03.015
Let's start with filter methods.

2
00:00:03.015 --> 00:00:05.370
Filter methods are one of

3
00:00:05.370 --> 00:00:08.070
the supervised methods of
doing feature selection,

4
00:00:08.070 --> 00:00:11.385
which includes wrapper methods
and embedded methods also.

5
00:00:11.385 --> 00:00:12.960
But for filter methods,

6
00:00:12.960 --> 00:00:15.150
we're primarily using correlation

7
00:00:15.150 --> 00:00:17.130
to look for the features that

8
00:00:17.130 --> 00:00:19.110
contain the information that

9
00:00:19.110 --> 00:00:21.585
we're going to use to
predict our target.

10
00:00:21.585 --> 00:00:24.360
Univariate feature
selection is also very

11
00:00:24.360 --> 00:00:27.525
often used for efficiency really.

12
00:00:27.525 --> 00:00:30.765
Let's take a look at
some filter methods.

13
00:00:30.765 --> 00:00:32.850
Correlated features are usually

14
00:00:32.850 --> 00:00:35.070
redundant as we
just talked about.

15
00:00:35.070 --> 00:00:36.600
When you have to
features that are

16
00:00:36.600 --> 00:00:38.550
highly correlated
with each other,

17
00:00:38.550 --> 00:00:41.865
that usually is an indicator
that they're redundant.

18
00:00:41.865 --> 00:00:43.140
You want to choose one of

19
00:00:43.140 --> 00:00:45.780
those so you remove
the other one.

20
00:00:45.780 --> 00:00:50.185
Popular filter methods
include Pearson correlation.

21
00:00:50.185 --> 00:00:52.815
These are different
ways to do correlation.

22
00:00:52.815 --> 00:00:54.725
Pearson correlation is one.

23
00:00:54.725 --> 00:00:56.990
That's correlation
between features and

24
00:00:56.990 --> 00:00:59.060
between the features
and the label.

25
00:00:59.060 --> 00:01:01.555
That's why it's a
supervised method.

26
00:01:01.555 --> 00:01:04.280
You can also do univariate
feature selection

27
00:01:04.280 --> 00:01:06.115
which is very common.

28
00:01:06.115 --> 00:01:11.660
Filter methods; well, we are
going to start with all of

29
00:01:11.660 --> 00:01:13.790
the features and
we're going to select

30
00:01:13.790 --> 00:01:16.280
the best subset and
we're going to give

31
00:01:16.280 --> 00:01:19.475
those to our model and
that's going to give us

32
00:01:19.475 --> 00:01:21.530
our performance
for the model with

33
00:01:21.530 --> 00:01:24.250
this subset of our features.

34
00:01:24.250 --> 00:01:26.210
One of the ways to visualize this

35
00:01:26.210 --> 00:01:28.190
is really correlation matrix.

36
00:01:28.190 --> 00:01:31.640
We can look for features where
we can see that there is

37
00:01:31.640 --> 00:01:35.345
a correlation between two
or more of our features.

38
00:01:35.345 --> 00:01:37.430
It helps show how
features are related,

39
00:01:37.430 --> 00:01:39.680
both to each other and with

40
00:01:39.680 --> 00:01:42.410
the target variable
and to emphasize that,

41
00:01:42.410 --> 00:01:45.000
when they're unrelated to
each other, that's bad.

42
00:01:45.000 --> 00:01:47.090
You only want one of
those and of course,

43
00:01:47.090 --> 00:01:49.670
you do want it correlated
with a target.

44
00:01:49.670 --> 00:01:52.550
That's going to fall in
a range of correlation

45
00:01:52.550 --> 00:01:55.140
between negative 1 and one and

46
00:01:55.140 --> 00:01:57.380
one is highly
positive correlation

47
00:01:57.380 --> 00:02:00.845
and negative 1 is highly
negative correlation.

48
00:02:00.845 --> 00:02:03.185
Comparing the different tests,

49
00:02:03.185 --> 00:02:07.085
you have Pearson's correlation
for linear relationships.

50
00:02:07.085 --> 00:02:09.920
That's by far I think
the most common.

51
00:02:09.920 --> 00:02:13.850
Kendall Tau is a rank
correlation coefficient,

52
00:02:13.850 --> 00:02:16.610
looks at monotonic relationships,

53
00:02:16.610 --> 00:02:18.230
and it's usually used with

54
00:02:18.230 --> 00:02:21.305
a fairly small sample
size for efficiency.

55
00:02:21.305 --> 00:02:23.630
Spearman's is another one.

56
00:02:23.630 --> 00:02:26.180
It's also rank correlation and it

57
00:02:26.180 --> 00:02:29.305
also is looking at
monotonic relationship.

58
00:02:29.305 --> 00:02:32.475
Other methods; well,
there's mutual information.

59
00:02:32.475 --> 00:02:35.015
Mutual information has some
nice characteristics to it.

60
00:02:35.015 --> 00:02:40.400
F-test, that's another fairly
common one and chi-squared.

61
00:02:40.400 --> 00:02:43.220
Chi-square is also a
fairly common one.

62
00:02:43.220 --> 00:02:46.055
Looking at how we
would do this in code,

63
00:02:46.055 --> 00:02:49.865
let's just take a look using
Pearson's correlation.

64
00:02:49.865 --> 00:02:51.425
This is going to use pandas.

65
00:02:51.425 --> 00:02:53.510
We have a data frame and we're

66
00:02:53.510 --> 00:02:56.885
using the
Pearsoncorrelation.cor method.

67
00:02:56.885 --> 00:03:00.615
That gives us our correlation
and we can then draw

68
00:03:00.615 --> 00:03:01.830
our heat map with

69
00:03:01.830 --> 00:03:04.560
seaborn and that's what
that's going to look like.

70
00:03:04.560 --> 00:03:07.545
That gives us our
correlation matrix.

71
00:03:07.545 --> 00:03:09.910
Looking at it again,

72
00:03:09.910 --> 00:03:11.570
we've got the absolute value of

73
00:03:11.570 --> 00:03:16.450
that and we can
select our target.

74
00:03:16.520 --> 00:03:20.220
That gives us our result with

75
00:03:20.220 --> 00:03:23.390
a subset of our features
that we've selected.

76
00:03:23.390 --> 00:03:24.890
In this case, we've eliminated

77
00:03:24.890 --> 00:03:27.235
seven features because
they were correlated.

78
00:03:27.235 --> 00:03:29.030
Now, we have instead of 30,

79
00:03:29.030 --> 00:03:31.160
we have 21 features in
our feature vector.

80
00:03:31.160 --> 00:03:35.435
The accuracy is actually better,

81
00:03:35.435 --> 00:03:37.610
the AUC is actually better,

82
00:03:37.610 --> 00:03:39.410
precision is actually better

83
00:03:39.410 --> 00:03:42.145
and recall is exactly the same.

84
00:03:42.145 --> 00:03:44.625
At least have five
decimal places.

85
00:03:44.625 --> 00:03:47.120
The F1 score is a little better.

86
00:03:47.120 --> 00:03:48.920
Removing features,

87
00:03:48.920 --> 00:03:52.980
mostly almost all our
metrics improved.

88
00:03:52.980 --> 00:03:55.010
That also goes along with

89
00:03:55.010 --> 00:03:57.470
the improvements that we made in

90
00:03:57.470 --> 00:03:59.780
the compute resources that are

91
00:03:59.780 --> 00:04:04.015
required to process 21
features instead of 30.

92
00:04:04.015 --> 00:04:07.440
That's our best result so far.

93
00:04:07.440 --> 00:04:10.675
Let's look at univariate
feature selection.

94
00:04:10.675 --> 00:04:13.280
Univariate feature selection and

95
00:04:13.280 --> 00:04:16.085
we're going to do that
using Sci-kit learn.

96
00:04:16.085 --> 00:04:21.425
There's SelectKBest,
there's SelectPercentile,

97
00:04:21.425 --> 00:04:23.630
GenericUnivariateSelect,

98
00:04:23.630 --> 00:04:26.425
which is fairly generic I assume.

99
00:04:26.425 --> 00:04:29.240
Some statistical tests
we can use with that,

100
00:04:29.240 --> 00:04:31.610
there's mutual information and

101
00:04:31.610 --> 00:04:34.420
F-tests for regression problems.

102
00:04:34.420 --> 00:04:36.020
For classification, we have

103
00:04:36.020 --> 00:04:39.110
chi-squared and
there's a version of

104
00:04:39.110 --> 00:04:41.255
the F-test for classification

105
00:04:41.255 --> 00:04:44.120
and similarly for
mutual information,

106
00:04:44.120 --> 00:04:47.315
there's a version of
that for classification.

107
00:04:47.315 --> 00:04:51.320
Let's look at how that
gets implemented in code.

108
00:04:51.320 --> 00:04:53.450
We've got a function
that we're going to

109
00:04:53.450 --> 00:04:55.445
define for univariate selection.

110
00:04:55.445 --> 00:04:57.935
It's going to take our test set,

111
00:04:57.935 --> 00:05:00.895
actually our training
set has been split for

112
00:05:00.895 --> 00:05:04.070
both training and
test and then that's

113
00:05:04.070 --> 00:05:06.080
going to be scaled
with a standard

114
00:05:06.080 --> 00:05:08.810
scalar and we're going to use

115
00:05:08.810 --> 00:05:12.080
the MinMaxScalar
as well to give us

116
00:05:12.080 --> 00:05:15.720
a scaled x and then our
selector we're going to

117
00:05:15.720 --> 00:05:20.350
use is the SelectKBest and
we're going to use that with

118
00:05:20.350 --> 00:05:23.480
the chi-squared test and that's

119
00:05:23.480 --> 00:05:27.095
going to look for the
20 best features here.

120
00:05:27.095 --> 00:05:29.645
We're going to fit
that transform,

121
00:05:29.645 --> 00:05:31.880
we're going to get the features

122
00:05:31.880 --> 00:05:33.920
that it selects
and we're going to

123
00:05:33.920 --> 00:05:38.600
drop the other features
in our original data set.

124
00:05:38.600 --> 00:05:41.210
That gives us the feature names

125
00:05:41.210 --> 00:05:43.840
of the features that
we've selected.

126
00:05:43.840 --> 00:05:45.885
How does that perform?

127
00:05:45.885 --> 00:05:49.220
Well, a univariate test
using chi-squared,

128
00:05:49.220 --> 00:05:50.930
we asked for 20 features,

129
00:05:50.930 --> 00:05:52.670
so it gave us 20 features.

130
00:05:52.670 --> 00:05:55.145
The accuracy dropped
a little bit,

131
00:05:55.145 --> 00:05:56.960
so did the AUC,

132
00:05:56.960 --> 00:05:58.550
so did the precision,

133
00:05:58.550 --> 00:06:00.515
but recall interestingly did not,

134
00:06:00.515 --> 00:06:04.070
its still exactly the
same and the F1 score

135
00:06:04.070 --> 00:06:06.425
is unfortunately a
little bit lower.

136
00:06:06.425 --> 00:06:09.635
The correlation is
still our best result.

137
00:06:09.635 --> 00:06:11.300
It's one more feature,

138
00:06:11.300 --> 00:06:14.130
but it's still our best result.