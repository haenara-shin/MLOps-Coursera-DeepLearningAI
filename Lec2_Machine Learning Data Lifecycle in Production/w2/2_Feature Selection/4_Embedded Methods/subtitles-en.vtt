WEBVTT

1
00:00:00.540 --> 00:00:02.551
Now let's look at some Embedded Methods.

2
00:00:03.640 --> 00:00:06.140
So, what are embedded methods?

3
00:00:06.140 --> 00:00:10.540
Well, it's a again,
a supervised method of Features Selection.

4
00:00:10.540 --> 00:00:12.491
And we've looked at Filter Methods.

5
00:00:12.491 --> 00:00:14.380
We've looked at Wrapper Methods.

6
00:00:14.380 --> 00:00:16.540
Let's look at Embedded Methods.

7
00:00:16.540 --> 00:00:22.000
So L1 or L2 regularization is
essentially an embedded method for

8
00:00:22.000 --> 00:00:24.640
doing feature selection.

9
00:00:24.640 --> 00:00:26.980
Feature importance is another method.

10
00:00:26.980 --> 00:00:31.540
Both of these are highly connected
to the model that you're using.

11
00:00:31.540 --> 00:00:35.699
So these both L1 regularization and
feature importance really sort of

12
00:00:35.699 --> 00:00:39.810
an intrinsic characteristic of
the model that you're working with.

13
00:00:39.810 --> 00:00:42.160
So it assigns scores.

14
00:00:42.160 --> 00:00:45.601
And for regularization really,
we're talking about waiting for

15
00:00:45.601 --> 00:00:47.440
each feature in the data.

16
00:00:47.440 --> 00:00:52.044
And it discards features often by
by setting those weights to zero or

17
00:00:52.044 --> 00:00:53.240
near zero.

18
00:00:53.240 --> 00:00:56.983
And that's going to eliminate the features
that we're talking about based on

19
00:00:56.983 --> 00:00:59.451
the feature essentially
the feature important.

20
00:01:00.540 --> 00:01:04.857
So look at that in SKLearn,
if we look at Feature Importance class,

21
00:01:04.857 --> 00:01:07.531
that's built into the Tree Based Model.

22
00:01:07.531 --> 00:01:10.613
So, again, we're still using
the RandomForestClassifier

23
00:01:10.613 --> 00:01:12.350
that we've been using all along.

24
00:01:12.350 --> 00:01:15.630
That's one of the model types that
does include feature importance.

25
00:01:15.630 --> 00:01:20.286
It's available as a property of that
model, the feature importances.

26
00:01:20.286 --> 00:01:23.902
And we can use SelectFromModel
to select the features

27
00:01:23.902 --> 00:01:28.335
from the train model based on
the assigned feature importances.

28
00:01:28.335 --> 00:01:34.440
So again, working with the model,
it's really a characteristic of the model.

29
00:01:34.440 --> 00:01:35.690
How does that look in code?

30
00:01:35.690 --> 00:01:38.941
Well, we're going to define a function.

31
00:01:38.941 --> 00:01:43.648
Feature importance is from
Tree Based Model and here's our data,

32
00:01:43.648 --> 00:01:48.880
it's been split into training and
test and we've separated our labels.

33
00:01:48.880 --> 00:01:51.608
We're going to use our
RandomForestClassifier as our model and

34
00:01:51.608 --> 00:01:53.140
we're going to fit it.

35
00:01:53.140 --> 00:01:56.940
And then we're going to pull out
the feature importances here.

36
00:01:56.940 --> 00:02:01.440
That's going to give us a series that
we're going to use a Panda series.

37
00:02:01.440 --> 00:02:05.610
That Panda series,
we're going to select the 10 best.

38
00:02:05.610 --> 00:02:10.440
So the 10 highest feature importances and
show that.

39
00:02:10.440 --> 00:02:14.282
So if we do that,
this is the visualization that we get and

40
00:02:14.282 --> 00:02:17.804
we can see the 10 best
features that we have based on

41
00:02:17.804 --> 00:02:22.140
the feature importance that
was calculated in the model.

42
00:02:22.140 --> 00:02:26.809
So using that to select those features,
we're going to go back and

43
00:02:26.809 --> 00:02:28.550
select from our model.

44
00:02:28.550 --> 00:02:30.570
That's going to give us our model and

45
00:02:30.570 --> 00:02:34.350
we're going to get support to get
the indexes for those features.

46
00:02:34.350 --> 00:02:37.521
And then we're going to drop the other
features from our feature vector.

47
00:02:37.521 --> 00:02:42.540
And that's going to give us the names
of the features that we're keeping.

48
00:02:42.540 --> 00:02:47.442
So tying those together, we've got feature
important from tree based model that

49
00:02:47.442 --> 00:02:51.940
that's going to give us the feature
importances and plot them.

50
00:02:51.940 --> 00:02:55.486
Then we're going to select the features
that we're going to keep and

51
00:02:55.486 --> 00:02:58.340
that's going to give us our performance.

52
00:02:58.340 --> 00:03:03.744
So in this case we've selected 14
features and looking at the metrics for

53
00:03:03.744 --> 00:03:08.831
them, they're actually not quite
as good but they're pretty good.

54
00:03:08.831 --> 00:03:10.060
So accuracy.

55
00:03:10.060 --> 00:03:12.540
It's now down a little bit.

56
00:03:12.540 --> 00:03:16.840
The ROC is up a little bit.

57
00:03:16.840 --> 00:03:22.049
It's kind of back to what it was
with all features are almost there.

58
00:03:22.049 --> 00:03:29.140
Precision is down a little bit but
still I mean it's down a little bit.

59
00:03:29.140 --> 00:03:34.230
Recall is not bad really
because pretty much the same.

60
00:03:34.230 --> 00:03:41.310
It is the same and the F1 Score is back
to what it was with all of the features.

61
00:03:41.310 --> 00:03:45.940
So Recursive Feature Elimination
is really still our best result.

62
00:03:45.940 --> 00:03:50.101
Although with Feature Importance we
were able to get down to 14 features.

63
00:03:50.101 --> 00:03:55.594
So this is a question where we need
to consider the importance of our

64
00:03:55.594 --> 00:04:01.210
metrics versus the importance of
reducing our compute resources.

65
00:04:01.210 --> 00:04:05.312
If it's really important, that compute
resources, we may want to do that

66
00:04:05.312 --> 00:04:09.540
with the 14 features that were
selected with Feature Importance.

67
00:04:09.540 --> 00:04:13.581
But if we're really interested in
maintaining those metrics were improving

68
00:04:13.581 --> 00:04:17.261
them, then Recursive Feature Elimination
is still the best result.

69
00:04:18.640 --> 00:04:19.741
Such review.

70
00:04:19.741 --> 00:04:25.540
We've covered a lot this week, we started
out with an Introduction to Preprocessing.

71
00:04:25.540 --> 00:04:29.861
We talked at some length about
Feature Engineering and why it's so

72
00:04:29.861 --> 00:04:34.900
important to doing machine learning,
especially in production settings.

73
00:04:34.900 --> 00:04:39.545
We looked at Preprocessing Data at Scale,
which is going to be important

74
00:04:39.545 --> 00:04:44.269
whenever you're working with large
amounts of data or large models and

75
00:04:44.269 --> 00:04:47.050
specifically at TensorFlow Transform.

76
00:04:47.050 --> 00:04:52.251
We also looked at Feature Spaces and
understanding how Feature Spaces work and

77
00:04:52.251 --> 00:04:56.220
and why we need to consider that
as we do Feature Selection.

78
00:04:56.220 --> 00:04:59.340
And we looked at different
kinds of Feature Selection.

79
00:04:59.340 --> 00:05:03.277
So Filter Methods and
Wrapper Methods and Embedded Methods.

80
00:05:03.277 --> 00:05:08.490
We've covered a lot about working with
our data and doing feature engineering.

81
00:05:08.490 --> 00:05:10.351
This has been quite a week so far.