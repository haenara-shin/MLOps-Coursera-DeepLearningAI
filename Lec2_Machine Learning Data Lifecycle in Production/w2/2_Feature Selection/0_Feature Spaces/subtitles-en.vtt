WEBVTT

1
00:00:00.640 --> 00:00:04.761
What are the important aspects
of shaping your data is

2
00:00:04.761 --> 00:00:09.140
to consider the feature
space that your data covers?

3
00:00:09.140 --> 00:00:10.840
Let's take a look.

4
00:00:10.840 --> 00:00:12.411
So here's what we're
going to be talking about.

5
00:00:12.411 --> 00:00:15.958
First of all just an introduction
to feature spaces and

6
00:00:15.958 --> 00:00:20.240
then an introduction to
feature selection as well.

7
00:00:20.240 --> 00:00:25.520
We'll talk about filter methods and
wrapper methods and embedded methods.

8
00:00:25.520 --> 00:00:28.030
Those are methods of feature selection.

9
00:00:28.030 --> 00:00:32.151
In this first section, we're really
just looking at feature spaces.

10
00:00:33.440 --> 00:00:35.320
So what are feature spaces?

11
00:00:35.320 --> 00:00:39.713
Well, a feature space is
defined by the n dimensional

12
00:00:39.713 --> 00:00:42.660
space that your features defined.

13
00:00:42.660 --> 00:00:47.750
So if you have two features,
a feature space is two dimensional.

14
00:00:47.750 --> 00:00:51.955
If you have three features,
its three dimensional and so forth,

15
00:00:51.955 --> 00:00:54.420
it does not include the target label.

16
00:00:54.420 --> 00:00:57.640
So that we're just talking
about your features.

17
00:00:57.640 --> 00:01:02.361
So if this is your feature factor,
you have future vector X.

18
00:01:02.361 --> 00:01:04.561
And it has values from zero to D.

19
00:01:04.561 --> 00:01:05.239
Here.

20
00:01:05.239 --> 00:01:08.440
That's the dimensionality of that vector.

21
00:01:08.440 --> 00:01:11.140
And that defines a space.

22
00:01:11.140 --> 00:01:13.621
So in this case that's a three D space.

23
00:01:13.621 --> 00:01:14.980
We have three features.

24
00:01:14.980 --> 00:01:15.801
We have a 3D.

25
00:01:15.801 --> 00:01:21.054
Space, or if we're looking at it in
a 2D space with with a two dimensional

26
00:01:21.054 --> 00:01:26.480
feature vector, we could express it
as a scatter plot with a 2D space.

27
00:01:26.480 --> 00:01:29.361
Those are fairly easy for
us as humans to imagine.

28
00:01:30.740 --> 00:01:32.780
So looking at it a little differently.

29
00:01:32.780 --> 00:01:37.748
Here's an example using features
that you might have for a house, so

30
00:01:37.748 --> 00:01:42.374
the number of rooms for the house or
actually any kind of building

31
00:01:42.374 --> 00:01:46.940
really the square footage of it
where it's located in a price.

32
00:01:46.940 --> 00:01:50.261
So maybe we're trying to
figure out how sales.

33
00:01:51.340 --> 00:01:56.457
So in this case,
the F is your model and it's acting

34
00:01:56.457 --> 00:02:02.407
in your feature space and
it's going to produce a result Y,

35
00:02:02.407 --> 00:02:08.970
looking at a classification situation or
problem we have again.

36
00:02:08.970 --> 00:02:12.040
So this is going to be
a two dimensional space.

37
00:02:12.040 --> 00:02:17.340
We have different distributions of
the examples within our feature space.

38
00:02:17.340 --> 00:02:20.820
In an ideal case, they're easy separable,

39
00:02:20.820 --> 00:02:25.240
ideally with a linear
function in a realistic case.

40
00:02:25.240 --> 00:02:29.861
Well, maybe we can do that with
a linear function if we sort of fudge

41
00:02:29.861 --> 00:02:33.738
around some of the some of
the examples and then we have,

42
00:02:33.738 --> 00:02:38.381
poor examples where it's difficult
to do with a linear function.

43
00:02:38.381 --> 00:02:41.835
We're working in a non linear
feature space and and,

44
00:02:41.835 --> 00:02:44.921
maybe we can use a projection
to help with that.

45
00:02:44.921 --> 00:02:49.624
Or maybe we just have to use
a non-linear model, but we're,

46
00:02:49.624 --> 00:02:55.340
what we're going to try to do is draw
a boundary within that feature space.

47
00:02:55.340 --> 00:03:01.140
So, here's a case where we're drawing,
maybe a parametric boundary.

48
00:03:01.140 --> 00:03:04.461
But the model is going to try
to learn that decision boundary.

49
00:03:05.740 --> 00:03:09.091
The boundary is used to
classify the data points.

50
00:03:09.091 --> 00:03:14.141
So that's what our model is learning as a
result of knowing where that boundary is.

51
00:03:14.141 --> 00:03:17.597
It can take an example
that is given to it and

52
00:03:17.597 --> 00:03:21.351
decide in this case which
class it falls into.

53
00:03:22.940 --> 00:03:25.260
So feature space coverage is important.

54
00:03:25.260 --> 00:03:30.574
The examples that you give to your
data to your model rather to train and

55
00:03:30.574 --> 00:03:33.908
for evaluation have to
be representative of

56
00:03:33.908 --> 00:03:38.881
the examples that you're going to
see when you serve your model.

57
00:03:38.881 --> 00:03:43.287
In other words, the requests that
are going to be given to your model and

58
00:03:43.287 --> 00:03:48.135
where they fall in your feature space,
that region of your feature space has to

59
00:03:48.135 --> 00:03:52.240
be covered when you train and
evaluate your model as well.

60
00:03:52.240 --> 00:03:57.218
So that means the same numerical ranges
were for classes the same classes and

61
00:03:57.218 --> 00:04:01.051
you need similar characteristics for
image data as well.

62
00:04:01.051 --> 00:04:05.131
So it's not quite so, numerical but

63
00:04:05.131 --> 00:04:12.140
it's still expressing a space and
similarly for vocabulary.

64
00:04:12.140 --> 00:04:16.944
In this case we have syntax and semantics
that we have to consider as well for

65
00:04:16.944 --> 00:04:18.861
natural language problems.

66
00:04:19.940 --> 00:04:24.540
So we need to ensure that
we covered the same space.

67
00:04:24.540 --> 00:04:27.180
If we're doing time series problem,

68
00:04:27.180 --> 00:04:31.040
we need to consider seasonality trend and
drift.

69
00:04:31.040 --> 00:04:33.932
And that includes for the serving data,

70
00:04:33.932 --> 00:04:38.440
we may have new values that we
find in new labels that's going to

71
00:04:38.440 --> 00:04:43.290
influence the concept drift as as
we've talked about previously,

72
00:04:43.290 --> 00:04:48.161
so we need to be aware of that and
design processes to deal with that.

73
00:04:49.300 --> 00:04:53.740
That means we need continuous monitoring,
which is going to be a key for

74
00:04:53.740 --> 00:04:55.790
success in these situations.

75
00:04:55.790 --> 00:05:00.328
Remember as we've talked about before,
the world changes and

76
00:05:00.328 --> 00:05:04.140
our model only learns one
version of the world.

77
00:05:04.140 --> 00:05:09.061
So when the world has changed, we need
to update our model to the new world.