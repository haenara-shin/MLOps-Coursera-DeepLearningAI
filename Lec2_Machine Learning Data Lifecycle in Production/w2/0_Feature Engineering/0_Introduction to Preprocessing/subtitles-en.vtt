WEBVTT

1
00:00:00.140 --> 00:00:05.240
Hello and welcome to feature engineering,
transformation and selection.

2
00:00:05.240 --> 00:00:10.074
Now, some of this material may seem like
it's review, especially from earlier

3
00:00:10.074 --> 00:00:14.010
courses or if you've worked in
an academic or research setting.

4
00:00:14.010 --> 00:00:18.594
But here, we're going to be
focusing on production issues,

5
00:00:18.594 --> 00:00:24.059
one of which means being able to do all
of this at scale in a reproducible and

6
00:00:24.059 --> 00:00:28.140
consistent way, so let's get started.

7
00:00:28.140 --> 00:00:32.928
Welcome to feature engineering,
transformation and selection,

8
00:00:32.928 --> 00:00:36.961
we'll start with an introduction
to pre-processing.

9
00:00:38.340 --> 00:00:43.725
There's a quote from Andrew Ng,
coming up with features is difficult,

10
00:00:43.725 --> 00:00:47.740
time consuming and
requires expert knowledge.

11
00:00:47.740 --> 00:00:53.494
Applied machine learning often requires
careful engineering of the features and

12
00:00:53.494 --> 00:00:54.251
data set.

13
00:00:55.340 --> 00:00:59.480
So now let's take a look at what we're
going to be talking about, first,

14
00:00:59.480 --> 00:01:02.723
we're going to look at how to
get the most out of our data and

15
00:01:02.723 --> 00:01:07.840
we'll take a look at the art and it
really, it is an art of feature engineer.

16
00:01:07.840 --> 00:01:11.753
We'll look at the future
engineering process itself and

17
00:01:11.753 --> 00:01:15.761
how feature engineering is
done in a typical ML pipeline.

18
00:01:16.940 --> 00:01:20.780
So let's start with how
we're going to squeeze

19
00:01:20.780 --> 00:01:24.440
information really out of our data.

20
00:01:24.440 --> 00:01:30.934
So, Emma models usually require some data
pre processing to improve training and you

21
00:01:30.934 --> 00:01:37.740
should probably by now have been training
models that this is very familiar to you.

22
00:01:37.740 --> 00:01:42.518
What may not be quite so familiar are some
of the issues that are involved in

23
00:01:42.518 --> 00:01:46.466
production environments, so
that's where we'll focus.

24
00:01:46.466 --> 00:01:51.231
The way that data is represented can
really have a big influence on how

25
00:01:51.231 --> 00:01:54.040
a model is able to learn from it.

26
00:01:54.040 --> 00:01:57.622
For example,
models tend to converge much faster and

27
00:01:57.622 --> 00:02:02.440
more reliably when numerical
data has been normalized.

28
00:02:02.440 --> 00:02:06.964
So techniques for selecting and
transforming the input data

29
00:02:06.964 --> 00:02:11.398
are key to increase the predictive
quality of the models and

30
00:02:11.398 --> 00:02:16.203
dimensionality reduction is
recommended whenever possible.

31
00:02:16.203 --> 00:02:19.838
So that the most relevant
information is preserved,

32
00:02:19.838 --> 00:02:24.686
while both the representation and
prediction ability are enhanced and

33
00:02:24.686 --> 00:02:28.340
the required compute
resources are reduced.

34
00:02:28.340 --> 00:02:32.641
Remember, in production ML
compute resources are a key

35
00:02:32.641 --> 00:02:36.210
contributor to the cost
of running a model, so

36
00:02:36.210 --> 00:02:41.450
let's kind of take a conceptual
look at feature engineering here.

37
00:02:41.450 --> 00:02:47.077
The art of feature engineering tries
to improve your model's ability

38
00:02:47.077 --> 00:02:53.083
to learn while reducing if possible,
the compute resources it requires,

39
00:02:53.083 --> 00:02:58.136
it does this by transforming and
projecting, eliminating and

40
00:02:58.136 --> 00:03:05.240
or combining the features in your raw data
to form a new version of your data set.

41
00:03:05.240 --> 00:03:08.442
So typically across the ML pipeline,

42
00:03:08.442 --> 00:03:13.546
you incorporate the original
features often transformed or

43
00:03:13.546 --> 00:03:18.861
projected to a new space and or
combinations of your features.

44
00:03:20.040 --> 00:03:25.021
Objective function must be properly tuned
to make sure your model is heading in

45
00:03:25.021 --> 00:03:30.540
the right direction and that is
consistent with your feature engineering.

46
00:03:30.540 --> 00:03:35.454
You can also update your model by
adding new features from the set

47
00:03:35.454 --> 00:03:39.822
of data that is available to
you unlike many things in ML,

48
00:03:39.822 --> 00:03:44.554
this tends to be an iterative
process that gradually improves

49
00:03:44.554 --> 00:03:48.220
your results as you iterate or
you hope it does.

50
00:03:48.220 --> 00:03:52.075
You have to monitor that and
if it's not improving,

51
00:03:52.075 --> 00:03:56.140
maybe back up and take another approach.

52
00:03:56.140 --> 00:04:01.512
Feature engineering is usually
applied in two fairly different ways,

53
00:04:01.512 --> 00:04:07.080
during training, you usually have
the entire data set available to you.

54
00:04:07.080 --> 00:04:11.222
So you can use global properties of
individual features in your feature

55
00:04:11.222 --> 00:04:13.640
engineering transformations.

56
00:04:13.640 --> 00:04:18.135
For example, you can compute
the standard deviation of a feature and

57
00:04:18.135 --> 00:04:21.440
then use that to perform normalization.

58
00:04:21.440 --> 00:04:24.725
However, when you serve
your trained model,

59
00:04:24.725 --> 00:04:27.759
you must do the same
feature engineering so

60
00:04:27.759 --> 00:04:33.040
that you give your model the same
types of data that it was trained on.

61
00:04:33.040 --> 00:04:36.080
For example,
if you created a one hot vector for

62
00:04:36.080 --> 00:04:38.740
a categorical feature when you trained,

63
00:04:38.740 --> 00:04:44.440
you need to also create an equivalent one
hot vector when you serve your model.

64
00:04:44.440 --> 00:04:47.303
However, during training and serving,

65
00:04:47.303 --> 00:04:52.540
you typically process each request
individually, so it's important that

66
00:04:52.540 --> 00:04:58.284
you include global properties of your
features, such as the standard deviation.

67
00:04:58.284 --> 00:05:03.516
If you use it during training include
that with the feature engineering

68
00:05:03.516 --> 00:05:08.834
that you do when serving, failing to
do that right is a very common source

69
00:05:08.834 --> 00:05:15.340
of problems in production systems, and
often these errors are difficult to find.

70
00:05:15.340 --> 00:05:20.617
So to review some key points,
as the quote from Andrew Ng demonstrates,

71
00:05:20.617 --> 00:05:23.991
feature engineering can
be very difficult and

72
00:05:23.991 --> 00:05:28.940
time consuming but
it is also very important to success.

73
00:05:28.940 --> 00:05:33.603
You want to squeeze the most out of
your data and you do that using feature

74
00:05:33.603 --> 00:05:38.940
engineering, by doing that,
you enable your models to learn better.

75
00:05:38.940 --> 00:05:43.964
You also want to make sure that you
concentrate predictive information,

76
00:05:43.964 --> 00:05:48.160
your data into as few features
as possible to make the best and

77
00:05:48.160 --> 00:05:52.340
least expensive use of
your compute resources.

78
00:05:52.340 --> 00:05:56.654
And you need to make sure that you apply
the same feature engineering during

79
00:05:56.654 --> 00:05:59.061
serving as you applied during training.