WEBVTT

1
00:00:00.020 --> 00:00:02.565
Hello and welcome back.

2
00:00:02.565 --> 00:00:06.270
You've probably heard the
phrase garbage in, garbage out.

3
00:00:06.270 --> 00:00:09.795
Well, that's very true
for Production ML.

4
00:00:09.795 --> 00:00:11.925
You can have a live data.

5
00:00:11.925 --> 00:00:13.590
But if your data isn't good,

6
00:00:13.590 --> 00:00:17.915
well it's not good for ML
either. How do you know?

7
00:00:17.915 --> 00:00:21.055
How do you know your data
is good or isn't good?

8
00:00:21.055 --> 00:00:23.655
Let's find out right now.

9
00:00:23.655 --> 00:00:28.790
Now let's look at validating
data and detecting

10
00:00:28.790 --> 00:00:31.940
data issues and really
look at trying to

11
00:00:31.940 --> 00:00:36.930
understand what issues
we need to deal with.

12
00:00:37.130 --> 00:00:40.980
We need to detect data issues,

13
00:00:40.980 --> 00:00:44.130
and we often talk
about drift and skew.

14
00:00:44.130 --> 00:00:47.240
We're going to be
discussing Data and concept

15
00:00:47.240 --> 00:00:50.690
Drift and Schema Skew
and Distribution Skew,

16
00:00:50.690 --> 00:00:56.694
and how to go about conceptually
detecting data issues.

17
00:00:56.694 --> 00:00:58.530
First of all, let's get

18
00:00:58.530 --> 00:01:01.740
some definitions out of
the way, drift and skew.

19
00:01:01.740 --> 00:01:06.090
Drift is changes
in data over time.

20
00:01:06.090 --> 00:01:10.760
For example, data collected
once a day over time,

21
00:01:10.760 --> 00:01:12.050
maybe a week later,

22
00:01:12.050 --> 00:01:16.430
a month later, there are
changes that data has drifted.

23
00:01:16.430 --> 00:01:19.610
Skew is the difference between

24
00:01:19.610 --> 00:01:22.985
two static versions from

25
00:01:22.985 --> 00:01:27.980
different sources of
conceptually the same dataset.

26
00:01:27.980 --> 00:01:30.140
For example, it could
be the difference

27
00:01:30.140 --> 00:01:32.825
between your training set

28
00:01:32.825 --> 00:01:35.420
and the data that
you're getting for

29
00:01:35.420 --> 00:01:38.090
prediction requests,
your serving set.

30
00:01:38.090 --> 00:01:42.480
Those differences are
referred to as skew.

31
00:01:42.500 --> 00:01:46.450
In a typical ML pipeline,

32
00:01:46.460 --> 00:01:49.400
this shows batch processing,

33
00:01:49.400 --> 00:01:51.515
but it could also be
online processing.

34
00:01:51.515 --> 00:01:54.170
You'll have different sources

35
00:01:54.170 --> 00:01:56.270
of data that are
conceptually the same.

36
00:01:56.270 --> 00:01:57.920
They have the same
feature vector,

37
00:01:57.920 --> 00:02:01.005
but over time they will change.

38
00:02:01.005 --> 00:02:03.740
That means that model
performance can either

39
00:02:03.740 --> 00:02:06.170
drop quickly due to things like

40
00:02:06.170 --> 00:02:10.010
system failure or can decay
over time due to changes

41
00:02:10.010 --> 00:02:14.390
in the data and things
like changes in the world.

42
00:02:14.390 --> 00:02:18.320
We're going to focus on
performance decay over time

43
00:02:18.320 --> 00:02:20.630
that arises due to issues

44
00:02:20.630 --> 00:02:23.435
between training
and serving data.

45
00:02:23.435 --> 00:02:25.970
There's really two
main reasons for that.

46
00:02:25.970 --> 00:02:29.435
There's Data drift, which
are changes in the data

47
00:02:29.435 --> 00:02:34.850
between training and serving
typically and Concept drift,

48
00:02:34.850 --> 00:02:36.050
which are changes in

49
00:02:36.050 --> 00:02:39.540
the world changes in
the ground truth.

50
00:02:40.010 --> 00:02:44.810
To understand model
decay over time,

51
00:02:44.810 --> 00:02:48.050
an ML model will start
to perform poorly

52
00:02:48.050 --> 00:02:53.140
in many cases and we refer
to this as model decay.

53
00:02:53.140 --> 00:02:56.720
That's usually or
often caused by drift,

54
00:02:56.720 --> 00:03:02.240
which is changes in
a conceptual way.

55
00:03:02.240 --> 00:03:04.070
There is changes in

56
00:03:04.070 --> 00:03:07.040
the statistical properties
of the features.

57
00:03:07.040 --> 00:03:10.040
It's sometimes due to
things like seasonality or

58
00:03:10.040 --> 00:03:13.340
trend or unexpected events,

59
00:03:13.340 --> 00:03:15.140
or just changes in the world.

60
00:03:15.140 --> 00:03:20.090
This example here, we're
looking at an app that during

61
00:03:20.090 --> 00:03:25.069
training the app
classified as a spammer,

62
00:03:25.069 --> 00:03:29.785
any user who is sending 20
or more messages per minute.

63
00:03:29.785 --> 00:03:35.510
We classified anybody
like that as a spammer.

64
00:03:35.510 --> 00:03:37.850
But after a system
update which you

65
00:03:37.850 --> 00:03:40.900
see as labeled on
the chart there,

66
00:03:40.900 --> 00:03:43.725
both spammers and non-spammers

67
00:03:43.725 --> 00:03:46.395
start to send more messages.

68
00:03:46.395 --> 00:03:48.810
In this case, the data,

69
00:03:48.810 --> 00:03:50.600
the world has changed and

70
00:03:50.600 --> 00:03:53.960
that causes unwanted
misclassification.

71
00:03:53.960 --> 00:03:56.390
We have all of our
users are classified as

72
00:03:56.390 --> 00:03:59.800
spammers which they
probably won't like.

73
00:03:59.800 --> 00:04:03.050
Concept drift is a change in

74
00:04:03.050 --> 00:04:07.615
the statistical properties
of the labels over time.

75
00:04:07.615 --> 00:04:11.510
At training, an ML
model learns a mapping

76
00:04:11.510 --> 00:04:15.005
between the features
and the labels.

77
00:04:15.005 --> 00:04:17.990
In a static world that's
fine, that won't change.

78
00:04:17.990 --> 00:04:20.630
But in real-world,

79
00:04:20.630 --> 00:04:25.655
distribution and the labels
meaning will change.

80
00:04:25.655 --> 00:04:29.390
The model needs to change
as well as the mapping

81
00:04:29.390 --> 00:04:33.835
found during training
will no longer be valid.

82
00:04:33.835 --> 00:04:35.920
As you previously see,

83
00:04:35.920 --> 00:04:37.670
there are many factors that cause

84
00:04:37.670 --> 00:04:39.410
changes over time including

85
00:04:39.410 --> 00:04:41.000
upstream data changes and

86
00:04:41.000 --> 00:04:45.100
seasonality and evolving
business processes.

87
00:04:45.100 --> 00:04:49.490
Schema skew occurs
when the training and

88
00:04:49.490 --> 00:04:51.470
scheming as serving data do

89
00:04:51.470 --> 00:04:53.780
not conform to the same schema,

90
00:04:53.780 --> 00:04:55.910
which you might think

91
00:04:55.910 --> 00:04:59.210
could never happen but
actually it can because

92
00:04:59.210 --> 00:05:01.790
you're collecting data
and things change and

93
00:05:01.790 --> 00:05:03.890
suddenly you're
getting an integer

94
00:05:03.890 --> 00:05:05.450
where are you expecting a float.

95
00:05:05.450 --> 00:05:07.010
Or you're getting
a string where you

96
00:05:07.010 --> 00:05:08.990
are expecting a category.

97
00:05:08.990 --> 00:05:12.185
Distributions skew
is a divergence

98
00:05:12.185 --> 00:05:15.425
of training and
serving data sets.

99
00:05:15.425 --> 00:05:19.265
The data set shift can
be really manifested

100
00:05:19.265 --> 00:05:22.940
by covariant and concept
and other types of shifts.

101
00:05:22.940 --> 00:05:24.580
We'll talk about
that in a second.

102
00:05:24.580 --> 00:05:28.475
Skew detection involves
continuous evaluation

103
00:05:28.475 --> 00:05:32.750
of data coming to your server
once you train your model.

104
00:05:32.750 --> 00:05:35.030
To detect these changes,

105
00:05:35.030 --> 00:05:38.000
you need continuous monitoring

106
00:05:38.000 --> 00:05:40.415
and evaluation of the data.

107
00:05:40.415 --> 00:05:43.955
Let's take a look at a
more rigorous definition

108
00:05:43.955 --> 00:05:48.040
of the drift and skew
that we're talking.

109
00:05:48.040 --> 00:05:53.390
Dataset shift occurs when the
joint probability of x are

110
00:05:53.390 --> 00:05:55.460
features and y are

111
00:05:55.460 --> 00:05:59.960
labels is not the same
during training and serving.

112
00:05:59.960 --> 00:06:03.040
The data has shifted over time.

113
00:06:03.040 --> 00:06:07.550
Covariate shift refers
to the change in

114
00:06:07.550 --> 00:06:09.980
distribution of the
input variables

115
00:06:09.980 --> 00:06:12.890
present in training
and serving data.

116
00:06:12.890 --> 00:06:16.925
In other words, it's where the
marginal distribution of x

117
00:06:16.925 --> 00:06:21.680
are features is not the same
during training and serving,

118
00:06:21.680 --> 00:06:26.700
but the conditional
distribution remains unchanged.

119
00:06:26.900 --> 00:06:30.380
Concept shift refers to

120
00:06:30.380 --> 00:06:32.600
a change in the
relationship between

121
00:06:32.600 --> 00:06:35.660
the input and output variables as

122
00:06:35.660 --> 00:06:36.740
opposed to the differences in

123
00:06:36.740 --> 00:06:39.140
the Data Distribution
or input itself.

124
00:06:39.140 --> 00:06:40.790
In other words, it's when

125
00:06:40.790 --> 00:06:43.550
the conditional
distribution of y are

126
00:06:43.550 --> 00:06:45.740
labels given x are

127
00:06:45.740 --> 00:06:50.119
features is not the same
during training and serving,

128
00:06:50.119 --> 00:06:52.730
but the marginal distribution of

129
00:06:52.730 --> 00:06:57.090
x are features remains unchanged.

130
00:06:57.740 --> 00:07:03.820
There's a straightforward
workflow to detect data skew.

131
00:07:03.820 --> 00:07:07.370
The first stage is looking
at training data and

132
00:07:07.370 --> 00:07:11.825
computing baseline statistics
and a reference schema.

133
00:07:11.825 --> 00:07:16.660
Then you do basically the
same with your serving data,

134
00:07:16.660 --> 00:07:19.900
you're going to generate
the descriptive statistics.

135
00:07:19.900 --> 00:07:22.380
Then you compare the two.

136
00:07:22.380 --> 00:07:24.050
You compare your serving

137
00:07:24.050 --> 00:07:26.615
baseline statistics
and instances.

138
00:07:26.615 --> 00:07:28.220
You check for differences

139
00:07:28.220 --> 00:07:30.815
between that and
your training data.

140
00:07:30.815 --> 00:07:34.195
You look for skew and drift.

141
00:07:34.195 --> 00:07:37.190
Significant changes
become anomalies

142
00:07:37.190 --> 00:07:39.110
and they'll trigger an alert.

143
00:07:39.110 --> 00:07:42.680
That alert goes to whoever's
monitoring system,

144
00:07:42.680 --> 00:07:46.250
that can either be a human
or another system to

145
00:07:46.250 --> 00:07:48.050
analyze the change and

146
00:07:48.050 --> 00:07:50.540
decide on the proper
course of action.

147
00:07:50.540 --> 00:07:53.000
That's got to be
the remediation of

148
00:07:53.000 --> 00:07:54.140
the way that you're going to fix

149
00:07:54.140 --> 00:07:56.580
and react to that problem.